diff --git a/coseda/wRMSD-grid-search/gandalfiterator.py b/coseda/wRMSD-grid-search/gandalfiterator.py
index b9e28a1..86d2645 100644
--- a/coseda/wRMSD-grid-search/gandalfiterator.py
+++ b/coseda/wRMSD-grid-search/gandalfiterator.py
@@ -460,19 +460,21 @@ class AdaptiveEngine:
 
     def setup_sources(self) -> None:
         """
-        Create overlays for each source and initialize controllers for all images.
+        Initialize controllers for all images (read seed shifts from source).
+        NOTE: We no longer create persistent overlays here; overlays are created per run.
         """
+        from overlay_h5 import read_seed_shifts_mm_from_src  # local import to avoid cycles
+
         for src in self.h5_sources:
-            overlay_path = os.path.join(self.overlays_dir, os.path.basename(src).replace(".h5", ".overlay.h5"))
-            N = create_overlay(src, overlay_path, use_vds=False)
-            sc = SourceContext(h5_src=src, overlay=overlay_path, N_images=N)
-            # read seeds from overlay (copied from src)
-            xs, ys = get_seed_shifts_mm(overlay_path)
+            # Read seed shifts (mm) from source HDF5 (or zeros if not present)
+            xs, ys = read_seed_shifts_mm_from_src(src)
+            N = len(xs)
+            sc = SourceContext(h5_src=src, overlay="", N_images=N)  # overlay filled per run
             controllers: Dict[int, ImageController] = {}
             for idx in range(N):
                 ctrl = ImageController(
                     h5_src=src,
-                    overlay=overlay_path,
+                    overlay="",  # filled per run
                     image_idx=idx,
                     geom_res_value=self.res_value,
                     seed_mm=(float(xs[idx]), float(ys[idx])),
@@ -482,6 +484,7 @@ class AdaptiveEngine:
             sc.controllers = controllers
             self.sources[src] = sc
 
+
     # ----- Wave construction -----
 
     def _collect_candidates_for_wave(self) -> Dict[str, List[Candidate]]:
@@ -502,50 +505,67 @@ class AdaptiveEngine:
 
     def _build_wave_files(self, grouped: Dict[str, List[Candidate]]) -> List[RunWave]:
         """
-        For each overlay group, create a run wave:
-          - write absolute shifts to overlay for the listed indices
-          - create run_X/list, stream path, and meta json
+        For each SOURCE (group), create ONE overlay in this run, ONE .lst, and ONE .stream.
+        Overlay lives in the run dir and is named <src>-overlay%04d.h5.
         """
         waves: List[RunWave] = []
         if not grouped:
             return waves
 
-        # new run id
+        # Bump run id and create run dir
         self.run_counter += 1
         run_id = self.run_counter
         run_dir = os.path.join(self.runs_dir, f"run_{run_id:04d}")
         os.makedirs(run_dir, exist_ok=True)
 
-        # We'll create one .lst per overlay group, but share run_id namespace
-        # If you prefer one global .lst across overlays, adapt here.
-        for overlay_path, candidates in grouped.items():
-            # order candidates deterministically by (h5_src, image_idx)
+        for overlay_key, candidates in grouped.items():
+            # overlay_key came from previous design; recover the source from first candidate
+            if not candidates:
+                continue
+            src_path = candidates[0].h5_src
+            base = os.path.basename(src_path).rsplit(".h5", 1)[0]
+            # New overlay path for THIS run & THIS source:
+            run_overlay = os.path.join(run_dir, f"{base}-overlay{run_id:04d}.h5")
+
+            # Create the overlay (VDS recommended)
+            create_overlay(src_path, run_overlay, use_vds=True)
+
+            # Deterministic order for candidates
             candidates.sort(key=lambda c: (c.h5_src, c.image_idx))
 
-            lst_path = os.path.join(run_dir, f"{os.path.basename(overlay_path)}.lst")
-            meta_path = os.path.join(run_dir, f"{os.path.basename(overlay_path)}_meta.json")
-            stream_path = os.path.join(run_dir, f"{os.path.basename(overlay_path)}.stream")
+            # These files belong to THIS run & THIS source
+            lst_path = os.path.join(run_dir, f"{base}-overlay{run_id:04d}.lst")
+            meta_path = os.path.join(run_dir, f"{base}-overlay{run_id:04d}_meta.json")
+            stream_path = os.path.join(run_dir, f"{base}-overlay{run_id:04d}.stream")
 
+            # Indices and absolute shifts (mm)
             indices = [c.image_idx for c in candidates]
             dx_mm = [c.abs_mm[0] for c in candidates]
             dy_mm = [c.abs_mm[1] for c in candidates]
 
-            # Write overlay shifts (absolute)
-            write_shifts_mm(overlay_path, indices, dx_mm, dy_mm)
-            # Write lst
-            write_lst(lst_path, overlay_path, indices)
-            # Write meta
+            # Write shifts to THIS run's overlay
+            write_shifts_mm(run_overlay, indices, dx_mm, dy_mm)
+
+            # IMPORTANT: this run uses run_overlay; update entries to carry the correct overlay path
+            # (so downstream matching finds the same filename that indexamajig writes)
+            from dataclasses import replace as dc_replace
+            entries = [dc_replace(c, overlay=run_overlay) for c in candidates]
+
+            # One .lst for THIS overlay
+            write_lst(lst_path, run_overlay, indices)
+
+            # Meta (purposes may differ per line, that's fine)
             meta = [
                 {
                     "h5_path": c.h5_src,
-                    "overlay": c.overlay,
+                    "overlay": run_overlay,
                     "image_idx": c.image_idx,
                     "purpose": c.purpose,
                     "seed_mm": [c.seed_mm[0], c.seed_mm[1]],
                     "delta_px": [c.delta_px[0], c.delta_px[1]],
                     "abs_shift_mm": [c.abs_mm[0], c.abs_mm[1]],
                 }
-                for c in candidates
+                for c in entries
             ]
             with open(meta_path, "w") as f:
                 json.dump(meta, f, indent=2)
@@ -555,19 +575,22 @@ class AdaptiveEngine:
                 lst_path=lst_path,
                 meta_path=meta_path,
                 stream_path=stream_path,
-                entries=candidates,
+                entries=entries,
             )
             waves.append(wave)
 
             # Log
             jsonl_append(self.log_path, {
                 "ts": time.time(), "type": "run_start",
-                "run_id": run_id, "overlay": overlay_path,
-                "lst_size": len(indices), "lst_path": lst_path
+                "run_id": run_id,
+                "overlay": run_overlay,
+                "lst_size": len(indices),
+                "lst_path": lst_path
             })
 
         return waves
 
+
 from gi_util import (
     jsonl_append,
     run_indexamajig,
@@ -593,70 +616,111 @@ def _read_text(path: str) -> str:
 class AdaptiveEngine(AdaptiveEngine):  # extend the class defined in Part 1
     # ----- Running waves -----
 
-    def _run_wave(self, wave: RunWave) -> RunResult:
-        # Execute indexamajig for this overlay-specific .lst
-        result = run_indexamajig(
+
+    def _run_wave(self, wave: RunWave) -> int:
+        """
+        Execute ONE run (ONE overlay + ONE .lst) and produce ONE .stream.
+        Returns indexamajig return code.
+        """
+        # Derive paths for the reproducer and stderr
+        run_dir = os.path.dirname(wave.lst_path)
+        base = os.path.splitext(os.path.basename(wave.stream_path))[0]   # e.g., <src>-overlay0001
+        sh_path = os.path.join(run_dir, f"{base}.sh")
+        err_path = os.path.join(run_dir, f"{base}.err")
+
+        # Optional: log the exact command we’re about to run
+        jsonl_append(self.log_path, {
+            "ts": time.time(), "type": "idx_cmd",
+            "lst": wave.lst_path, "geom": self.geom_path, "cell": self.cell_path,
+            "stream": wave.stream_path, "script": sh_path
+        })
+
+        rc = run_indexamajig(
             lst_path=wave.lst_path,
-            geom_path=os.path.join(self.inputs_dir, os.path.basename(self.geom_path)),
-            cell_path=os.path.join(self.inputs_dir, os.path.basename(self.cell_path)),
+            geom_path=self.geom_path,
+            cell_path=self.cell_path,
             out_stream_path=wave.stream_path,
-            flags_passthrough=self.idx_flags,
+            flags=self.indexamajig_flags,
+            log_jsonl_path=self.log_path,
+            run_script_path=sh_path,   # <-- write the .sh reproducer here
+            stderr_path=err_path,      # <-- capture stderr to file
         )
-        # log
-        jsonl_append(self.log_path, {
-            "ts": time.time(), "type": "run_end",
-            "run_id": wave.run_id, "overlay": os.path.basename(wave.lst_path).replace(".lst", ""),
-            "lst_size": len(wave.entries),
-            "elapsed_s": result.elapsed_s,
-            "returncode": result.returncode,
-            "stderr_tail": result.stderr_tail,
-            "stream_path": wave.stream_path,
-        })
-        return result
+        return rc
 
+    
     def _process_wave_results(self, wave: RunWave) -> None:
         """
-        For each candidate in the wave, try to find its chunk in the .stream;
-        if found, score wRMSD and update controller; else mark as not indexed.
+        For each candidate in this overlay-run, look up its chunk in the run's .stream.
+        If found: score wRMSD, update controller, and remember this exact stream path.
+        If not: mark as not indexed for this attempt (controller will propose next step).
         """
-        # Load stream text once
-        if not os.path.exists(wave.stream_path):
-            # No stream produced (crash); mark all as failed attempt
+        # If indexamajig didn't produce a stream, mark all as failed attempt
+        if not os.path.exists(wave.stream_path) or os.path.getsize(wave.stream_path) == 0:
             for cand in wave.entries:
                 ctrl = self.sources[cand.h5_src].controllers[cand.image_idx]
                 ctrl.incorporate_result(cand, indexed=False, metrics=None, params=self.params)
                 jsonl_append(self.log_path, {
                     "ts": time.time(), "type": "candidate_result",
                     "h5": cand.h5_src, "idx": cand.image_idx, "purpose": cand.purpose,
-                    "indexed": False, "reason": "no_stream"
+                    "indexed": False, "reason": "no_stream_or_empty",
+                    "stream": wave.stream_path
                 })
             return
 
-        text = _read_text(wave.stream_path)
+        # Load stream text once; we will slice per chunk
+        try:
+            with open(wave.stream_path, "r", encoding="utf-8", errors="ignore") as f:
+                stream_text = f.read()
+        except Exception as e:
+            # Be conservative: treat as a missing stream
+            for cand in wave.entries:
+                ctrl = self.sources[cand.h5_src].controllers[cand.image_idx]
+                ctrl.incorporate_result(cand, indexed=False, metrics=None, params=self.params)
+                jsonl_append(self.log_path, {
+                    "ts": time.time(), "type": "candidate_result",
+                    "h5": cand.h5_src, "idx": cand.image_idx, "purpose": cand.purpose,
+                    "indexed": False, "reason": f"stream_read_error:{e}",
+                    "stream": wave.stream_path
+                })
+            return
 
         for cand in wave.entries:
             ctrl = self.sources[cand.h5_src].controllers[cand.image_idx]
+
+            # Try to find the exact chunk for (overlay in this run, image_idx).
+            # We pass allow_off_by_one=True to be tolerant of Event numbering differences.
             span = find_chunk_span_for_image(
                 stream_path=wave.stream_path,
-                overlay_path=cand.overlay,
+                overlay_path=cand.overlay,            # per-run overlay path
                 image_idx=cand.image_idx,
+                source_path=cand.h5_src,              # tolerate CrystFEL writing source path
+                allow_off_by_one=True
             )
 
             if span is None:
-                # Not indexed / not present
+                # Not indexed / not present for this attempt
                 ctrl.incorporate_result(cand, indexed=False, metrics=None, params=self.params)
                 jsonl_append(self.log_path, {
                     "ts": time.time(), "type": "candidate_result",
                     "h5": cand.h5_src, "idx": cand.image_idx, "purpose": cand.purpose,
-                    "indexed": False
+                    "indexed": False, "stream": wave.stream_path
                 })
                 continue
 
-            # Score the chunk using text slice (avoids temp files)
-            chunk_text = text[span[0]:span[1]]
+            # Score the matched chunk
+            s, e = span
+            chunk_text = stream_text[s:e]
             try:
                 metrics = score_single_chunk_text(chunk_text)
                 ctrl.incorporate_result(cand, indexed=True, metrics=metrics, params=self.params)
+
+                # Remember exactly which stream produced this success
+                ctrl.last_success_stream_path = wave.stream_path
+
+                # Ensure the controller carries the current per-run overlay path (useful for later extraction)
+                if not ctrl.overlay or os.path.basename(ctrl.overlay) != os.path.basename(cand.overlay):
+                    ctrl.overlay = cand.overlay
+
                 jsonl_append(self.log_path, {
                     "ts": time.time(), "type": "candidate_result",
                     "h5": cand.h5_src, "idx": cand.image_idx, "purpose": cand.purpose,
@@ -664,14 +728,16 @@ class AdaptiveEngine(AdaptiveEngine):  # extend the class defined in Part 1
                     "n_reflections": metrics.get("n_reflections"),
                     "n_peaks": metrics.get("n_peaks"),
                     "cell_dev_pct": metrics.get("cell_dev_pct"),
+                    "stream": wave.stream_path
                 })
             except Exception as e:
-                # If scoring failed, treat as not indexed (conservative)
+                # Scoring failed → treat as not indexed to stay conservative
                 ctrl.incorporate_result(cand, indexed=False, metrics=None, params=self.params)
                 jsonl_append(self.log_path, {
                     "ts": time.time(), "type": "candidate_result",
                     "h5": cand.h5_src, "idx": cand.image_idx, "purpose": cand.purpose,
-                    "indexed": False, "reason": f"score_error:{e}"
+                    "indexed": False, "reason": f"score_error:{e}",
+                    "stream": wave.stream_path
                 })
 
     # ----- Completion checks & finalization -----
@@ -685,51 +751,179 @@ class AdaptiveEngine(AdaptiveEngine):  # extend the class defined in Part 1
 
     def _finalize_winners(self) -> List[str]:
         """
-        Extract per-image winner streams for all FINAL images that don't yet
-        have a streams/<src>_<idx>.stream file. Return list of paths created.
+        Extract per-image winner streams for images with FINAL state (only).
+        Returns list of paths created during this call.
         """
-        winner_paths: List[str] = []
-
-        # Iterate runs newest to oldest to find the chunk where best was achieved
-        # Simpler approach: try each run stream until we can extract a chunk at 'best' (overlay, idx).
-        # For performance, we could track last-success run, but correctness first.
-        run_dirs = sorted(
-            [os.path.join(self.runs_dir, d) for d in os.listdir(self.runs_dir) if d.startswith("run_")],
-            key=lambda p: p, reverse=True
-        )
+        os.makedirs(self.streams_dir, exist_ok=True)
+        created: List[str] = []
 
         for sc in self.sources.values():
             for idx, ctrl in sc.controllers.items():
                 if ctrl.state != ImgState.FINAL or ctrl.best is None:
                     continue
-                out_name = f"{os.path.basename(sc.h5_src).replace('.h5','')}_{idx}.stream"
+
+                out_name = f"{os.path.basename(sc.h5_src).rsplit('.h5',1)[0]}_{idx}.stream"
                 out_path = os.path.join(self.streams_dir, out_name)
                 if os.path.exists(out_path):
-                    continue  # already extracted
-
-                # Find the stream where this final-best was produced:
-                # We search backward over run streams for this overlay that contain this (overlay, idx) chunk.
-                found = False
-                for run_dir in run_dirs:
-                    # Streams are named per overlay in our design
-                    candidate_stream = os.path.join(run_dir, f"{os.path.basename(sc.overlay)}.stream")
-                    if not os.path.exists(candidate_stream):
-                        continue
-                    span = find_chunk_span_for_image(candidate_stream, sc.overlay, idx)
-                    if span is None:
-                        continue
-                    try:
-                        extract_winner_stream(candidate_stream, out_path, span)
-                        ctrl.winner_stream_path = out_path
-                        winner_paths.append(out_path)
-                        found = True
-                        break
-                    except Exception as e:
-                        # Try older streams if extraction fails
-                        continue
-
-                # If not found, we can skip (image may have finalized without a valid chunk, rare)
-        return winner_paths
+                    continue
+
+                cand_stream = getattr(ctrl, "last_success_stream_path", None)
+                overlay_hint = ctrl.overlay if getattr(ctrl, "overlay", "") else ""
+
+                span = None
+                if cand_stream and os.path.exists(cand_stream):
+                    span = find_chunk_span_for_image(
+                        stream_path=cand_stream,
+                        overlay_path=overlay_hint,
+                        image_idx=idx,
+                        source_path=sc.h5_src,
+                        allow_off_by_one=True
+                    )
+
+                if span is None:
+                    # Fallback search across run streams (most-recent-first)
+                    run_dirs = sorted(
+                        [os.path.join(self.runs_dir, d) for d in os.listdir(self.runs_dir) if d.startswith("run_")],
+                        key=lambda p: p, reverse=True
+                    )
+                    for run_dir in run_dirs:
+                        for nm in os.listdir(run_dir):
+                            if not nm.endswith(".stream"):
+                                continue
+                            sp = os.path.join(run_dir, nm)
+                            span = find_chunk_span_for_image(
+                                stream_path=sp,
+                                overlay_path=overlay_hint,
+                                image_idx=idx,
+                                source_path=sc.h5_src,
+                                allow_off_by_one=True
+                            )
+                            if span is not None:
+                                cand_stream = sp
+                                break
+                        if span is not None:
+                            break
+
+                if span is None or not cand_stream or not os.path.exists(cand_stream):
+                    # No extractable chunk yet (unexpected for FINAL, but tolerate)
+                    continue
+
+                try:
+                    extract_winner_stream(cand_stream, out_path, span)
+                    ctrl.winner_stream_path = out_path
+                    created.append(out_path)
+                except Exception as e:
+                    jsonl_append(self.log_path, {
+                        "ts": time.time(), "type": "winner_extract_error",
+                        "h5": sc.h5_src, "idx": idx, "err": str(e),
+                        "source_stream": cand_stream
+                    })
+
+        return created
+
+    def _finalize_current_winners(self, allow_nonfinal: bool = True) -> int:
+        """
+        Extract per-image best-so-far chunks into per-image winner streams under self.streams_dir.
+        - If allow_nonfinal=True: include any image with a seen 'best' (even if not FINAL yet).
+        - If allow_nonfinal=False: only include images whose state is FINAL.
+
+        Returns:
+            total number of per-image winner streams present after this call.
+        """
+        os.makedirs(self.streams_dir, exist_ok=True)
+
+        # Track what's already extracted to avoid repeated work
+        existing = set()
+        for nm in os.listdir(self.streams_dir):
+            if nm.endswith(".stream"):
+                existing.add(os.path.join(self.streams_dir, nm))
+
+        for sc in self.sources.values():
+            for idx, ctrl in sc.controllers.items():
+                if ctrl.best is None:
+                    continue
+                if (not allow_nonfinal) and (ctrl.state != ImgState.FINAL):
+                    continue
+
+                out_name = f"{os.path.basename(sc.h5_src).rsplit('.h5',1)[0]}_{idx}.stream"
+                out_path = os.path.join(self.streams_dir, out_name)
+                if out_path in existing or os.path.exists(out_path):
+                    continue
+
+                # Prefer the exact stream that produced the last success
+                cand_stream = getattr(ctrl, "last_success_stream_path", None)
+
+                # We need to find the chunk in that stream; use the controller's current overlay path if present
+                overlay_hint = ctrl.overlay if getattr(ctrl, "overlay", "") else ""
+
+                span = None
+                if cand_stream and os.path.exists(cand_stream):
+                    span = find_chunk_span_for_image(
+                        stream_path=cand_stream,
+                        overlay_path=overlay_hint,
+                        image_idx=idx,
+                        source_path=sc.h5_src,
+                        allow_off_by_one=True
+                    )
+
+                # Fallback: search recent run streams if we didn't find it on the remembered stream
+                if span is None:
+                    run_dirs = sorted(
+                        [os.path.join(self.runs_dir, d) for d in os.listdir(self.runs_dir) if d.startswith("run_")],
+                        key=lambda p: p, reverse=True
+                    )
+                    for run_dir in run_dirs:
+                        # Try any stream in this run directory
+                        for nm in os.listdir(run_dir):
+                            if not nm.endswith(".stream"):
+                                continue
+                            sp = os.path.join(run_dir, nm)
+                            span = find_chunk_span_for_image(
+                                stream_path=sp,
+                                overlay_path=overlay_hint,
+                                image_idx=idx,
+                                source_path=sc.h5_src,
+                                allow_off_by_one=True
+                            )
+                            if span is not None:
+                                cand_stream = sp
+                                break
+                        if span is not None:
+                            break
+
+                if span is None or not cand_stream or not os.path.exists(cand_stream):
+                    # We don't yet have a successful chunk on disk for this image; skip for now
+                    continue
+
+                try:
+                    extract_winner_stream(cand_stream, out_path, span)
+                    ctrl.winner_stream_path = out_path
+                    existing.add(out_path)
+                except Exception as e:
+                    # Skip on extraction failure; later waves may still succeed
+                    jsonl_append(self.log_path, {
+                        "ts": time.time(), "type": "winner_extract_error",
+                        "h5": sc.h5_src, "idx": idx, "err": str(e),
+                        "source_stream": cand_stream
+                    })
+
+        return len(existing)
+
+
+    def _merge_early_break(self, run_id: int) -> Optional[str]:
+        """Merge whatever per-image streams exist into an early-break stream for this run."""
+        streams = []
+        if not os.path.isdir(self.streams_dir):
+            return None
+        for nm in sorted(os.listdir(self.streams_dir)):
+            if nm.endswith(".stream"):
+                streams.append(os.path.join(self.streams_dir, nm))
+        if not streams:
+            return None
+        out_path = os.path.join(self.merged_dir, f"early_break_run_{run_id:04d}.stream")
+        merge_winner_streams(streams, out_path)
+        return out_path
+
 
     def _write_best_centers_csv(self) -> None:
         """
@@ -811,6 +1005,8 @@ class AdaptiveEngine(AdaptiveEngine):  # extend the class defined in Part 1
                 result = self._run_wave(wave)
                 # Process results even if returncode != 0 (partial outputs may exist)
                 self._process_wave_results(wave)
+                self._finalize_current_winners(allow_nonfinal=True)
+                self._merge_early_break(run_id=wave.run_id)
 
             # Optional pruning: delete old run streams whose images are all finalized
             # (We keep them until the very end for robust extraction.)
diff --git a/coseda/wRMSD-grid-search/gandalfiterator_window.py b/coseda/wRMSD-grid-search/gandalfiterator_window.py
index edb696e..004c185 100644
--- a/coseda/wRMSD-grid-search/gandalfiterator_window.py
+++ b/coseda/wRMSD-grid-search/gandalfiterator_window.py
@@ -22,8 +22,10 @@ import os
 import sys
 import json
 import shlex
-import time
+import subprocess
 from typing import List, Optional
+from pathlib import Path
+
 
 from PyQt6 import QtCore, QtGui, QtWidgets
 
@@ -140,6 +142,16 @@ class MainWindow(QtWidgets.QMainWindow):
         self.h5List = QtWidgets.QListWidget()
         self.h5List.setSelectionMode(QtWidgets.QAbstractItemView.SelectionMode.ExtendedSelection)
 
+        # ---- Defaults for debugging (comment out later) ----
+        DEBUG = True
+        if DEBUG:
+            default_root = ("/home/bubl3932/files/grid-search-wRMSD-optimization")
+            self.runRootEdit.setText(default_root)
+            self.geomEdit.setText(default_root + "/MFM.geom")
+            self.cellEdit.setText(default_root + "/MFM.cell")
+            self.h5List.addItem(default_root + "/MFM.h5")
+        # ---------------------------------------------------
+
         btnRunRoot = QtWidgets.QPushButton("Browse…")
         btnGeom = QtWidgets.QPushButton("Browse…")
         btnCell = QtWidgets.QPushButton("Browse…")
@@ -174,13 +186,14 @@ class MainWindow(QtWidgets.QMainWindow):
         grid = QtWidgets.QGridLayout(grp_params)
         grid.setContentsMargins(9,9,9,9)
 
+
         def spinD(default, step, decimals=3, minimum=-1e9, maximum=1e9):
             s = QtWidgets.QDoubleSpinBox()
             s.setDecimals(decimals)
             s.setSingleStep(step)
             s.setRange(minimum, maximum)
             s.setValue(default)
-            s.setMaximumWidth(120)
+            s.setMaximumWidth(120) 
             return s
 
         def spinI(default, step, minimum=1, maximum=10**9):
@@ -292,6 +305,68 @@ class MainWindow(QtWidgets.QMainWindow):
         self.h5List.clear()
 
     # ---- Run handling ----
+
+    def _ensure_run_bundle(
+        self,
+        lst_entries,
+        geom_path,
+        cell_path,
+        overlay_path,
+        run_dir,
+        script_path,
+        idx_exec,
+        extra_args=None,
+    ):
+        run_dir = Path(run_dir)
+        run_dir.mkdir(parents=True, exist_ok=True)
+
+        lst_path = run_dir / Path(overlay_path).with_suffix(".lst").name
+        stream_path = run_dir / Path(overlay_path).with_suffix(".stream").name
+        script_path = Path(script_path)
+
+        # 1) Write the .lst explicitly
+        with open(lst_path, "w") as f:
+            for p in lst_entries:
+                f.write(str(p) + "\n")
+
+        # 2) Build the exact command (absolute paths)
+        cmd = [
+            str(Path(idx_exec).resolve()),
+            "--lst", str(lst_path.resolve()),
+            "--geom", str(Path(geom_path).resolve()),
+            "--cell", str(Path(cell_path).resolve()),
+            "--out", str(stream_path.resolve()),
+        ]
+        if extra_args:
+            cmd.extend(extra_args)
+
+        # 3) Write a runnable .sh for inspection
+        with open(script_path, "w") as sh:
+            sh.write("#!/usr/bin/env bash\nset -euo pipefail\n")
+            sh.write("echo '[idx] CWD:' $(pwd)\n")
+            sh.write("echo '[idx] whoami:' $(whoami)\n")
+            sh.write("echo '[idx] running:' " + " ".join(shlex.quote(c) for c in cmd) + "\n")
+            sh.write(" ".join(shlex.quote(c) for c in cmd) + "\n")
+        os.chmod(script_path, 0o755)
+
+        return {
+            "lst": str(lst_path),
+            "stream": str(stream_path),
+            "script": str(script_path),
+            "cmd": cmd,
+        }
+
+    def _run_index_command(self, cmd, run_dir):
+        proc = subprocess.run(
+            cmd,
+            cwd=str(run_dir),
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            text=True,
+        )
+        return proc.returncode, proc.stdout, proc.stderr
+
+
     def _collectParams(self) -> Params:
         return Params(
             R_px=float(self.R_px.value()),
diff --git a/coseda/wRMSD-grid-search/gi_util.py b/coseda/wRMSD-grid-search/gi_util.py
index cd54593..5891328 100644
--- a/coseda/wRMSD-grid-search/gi_util.py
+++ b/coseda/wRMSD-grid-search/gi_util.py
@@ -92,33 +92,89 @@ class RunResult:
     elapsed_s: float
     stderr_tail: str
 
+def _shlex_join(parts: List[str]) -> str:
+    # py3.9+: shlex.join exists; keep a tiny reimpl to be safe
+    return " ".join(shlex.quote(p) for p in parts)
 
 def run_indexamajig(
     lst_path: str,
     geom_path: str,
     cell_path: str,
     out_stream_path: str,
-    flags_passthrough: Sequence[str],
-) -> RunResult:
+    flags: Optional[List[str]] = None,
+    log_jsonl_path: Optional[str] = None,
+    run_script_path: Optional[str] = None,   # <-- NEW: where to write the .sh
+    stderr_path: Optional[str] = None,       # <-- NEW: capture stderr to file
+) -> int:
     """
-    Run indexamajig with user's flags as-is.
-    We redirect stdout to the stream file and capture only stderr (tail) for logs.
+    Execute indexamajig for a single run (one overlay + one .lst).
+    Also emits a .sh reproducer if run_script_path is provided.
+
+    Returns the indexamajig return code.
     """
-    cmd = ["indexamajig", "-i", lst_path, "-g", geom_path, "-p", cell_path]
-    # pass-through flags verbatim (e.g., "-j", "32", "--peaks", "...", ...)
-    cmd.extend(flags_passthrough)
-
-    os.makedirs(os.path.dirname(os.path.abspath(out_stream_path)), exist_ok=True)
-    t0 = time.time()
-    with open(out_stream_path, "w") as stream_out, subprocess.Popen(
-        cmd,
-        stdout=stream_out,
-        stderr=subprocess.PIPE,
-        text=True,
-    ) as proc:
-        stderr = proc.communicate()[1] or ""
-        elapsed = time.time() - t0
-
-    # Keep only last few lines of stderr for quick diagnosis
-    tail = "\n".join(stderr.strip().splitlines()[-20:])
-    return RunResult(stream_path=out_stream_path, returncode=proc.returncode, elapsed_s=elapsed, stderr_tail=tail)
+    flags = flags or []
+    cmd = ["indexamajig", "-i", lst_path, "-g", geom_path, "-p", cell_path, *flags]
+
+    # Prepare stderr file
+    if stderr_path is None:
+        stderr_path = out_stream_path.replace(".stream", ".err")
+
+    # Emit a .sh reproducer for convenience
+    if run_script_path:
+        # We redirect in the shell for exact reproduction
+        script = f"""#!/usr/bin/env bash
+set -euo pipefail
+
+# Auto-generated by gi_util.run_indexamajig on {time.strftime('%Y-%m-%d %H:%M:%S')}
+# Reproduce the run for this overlay/.lst
+# LST:    {lst_path}
+# GEOM:   {geom_path}
+# CELL:   {cell_path}
+# STREAM: {out_stream_path}
+# STDERR: {stderr_path}
+
+echo "indexamajig version:"
+indexamajig --version || true
+echo
+
+CMD="{_shlex_join(cmd)}"
+echo "$CMD"
+# Run and capture outputs exactly as the engine did:
+$CMD > {shlex.quote(out_stream_path)} 2> {shlex.quote(stderr_path)}
+"""
+        os.makedirs(os.path.dirname(run_script_path), exist_ok=True)
+        with open(run_script_path, "w") as f:
+            f.write(script)
+        os.chmod(run_script_path, 0o755)
+
+    # Now run for real (no shell), and capture to the same files
+    os.makedirs(os.path.dirname(out_stream_path), exist_ok=True)
+    with open(out_stream_path, "w") as out_f, open(stderr_path, "w") as err_f:
+        proc = subprocess.Popen(cmd, stdout=out_f, stderr=err_f)
+        rc = proc.wait()
+
+    # Optional: log tail for quick triage
+    if log_jsonl_path:
+        try:
+            tail = ""
+            with open(stderr_path, "r", encoding="utf-8", errors="ignore") as ef:
+                tail = "".join(ef.readlines()[-50:])
+        except Exception:
+            pass
+        from json import dumps
+        with open(log_jsonl_path, "a") as lj:
+            lj.write(dumps({
+                "ts": time.time(),
+                "type": "run_end",
+                "lst": lst_path,
+                "geom": geom_path,
+                "cell": cell_path,
+                "stream": out_stream_path,
+                "stderr": stderr_path,
+                "returncode": rc,
+                "stderr_tail": tail,
+                "cmd": cmd,
+                "reproducer": run_script_path or "",
+            }) + "\n")
+
+    return rc
diff --git a/coseda/wRMSD-grid-search/overlay_h5.py b/coseda/wRMSD-grid-search/overlay_h5.py
index 9577b43..d47fc70 100644
--- a/coseda/wRMSD-grid-search/overlay_h5.py
+++ b/coseda/wRMSD-grid-search/overlay_h5.py
@@ -54,7 +54,7 @@ def _copy_initial_seeds(src: h5py.File, ov: h5py.File) -> None:
 def create_overlay(
     h5_src_path: str,
     h5_overlay_path: str,
-    use_vds: bool = False,
+    use_vds: bool = True,
 ) -> int:
     """
     Create overlay file (overwrite if exists). Returns number of images N.
@@ -157,3 +157,19 @@ def zero_shifts(
             idx = np.asarray(indices, dtype=int)
             ov[SHIFT_X_DS][idx] = 0.0
             ov[SHIFT_Y_DS][idx] = 0.0
+            
+def read_seed_shifts_mm_from_src(h5_src_path: str) -> Tuple[np.ndarray, np.ndarray]:
+    """
+    Read seed shifts directly from the source HDF5. If not present, return zeros.
+    """
+    h5_src_path = os.path.abspath(h5_src_path)
+    with h5py.File(h5_src_path, "r") as f:
+        N = f[IMAGES_DS].shape[0]
+        if SHIFT_X_DS in f and SHIFT_Y_DS in f:
+            xs = f[SHIFT_X_DS][...]
+            ys = f[SHIFT_Y_DS][...]
+            if xs.shape != (N,) or ys.shape != (N,):
+                raise ValueError(f"Seed arrays in source have wrong shape: {xs.shape}, {ys.shape}; expected {(N,)}")
+            return xs, ys
+        else:
+            return np.zeros((N,), dtype="f8"), np.zeros((N,), dtype="f8")
